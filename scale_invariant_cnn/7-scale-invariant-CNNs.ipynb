{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"7-scale-invariant-CNNs.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMyUu0PCgasjoae6h6s6+6q"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"DgJzF5aNOTdW"},"source":["## Code for implementing scale invariant CNNs.\n","\n","Here are the results of scale invariance CNNs applied on MNIST. The train images are taken from MNIST directly where as test images are scaled by a factor sampled uniformly from [1,1.5].\n","\n","```\n","RESULTS OF MULTISCALE CNN\n","Train accuracy of the model: 99.090 %\n","tensor(59455, device='cuda:0') 60000\n","Test accuracy of the model: 95.390 %\n","tensor(9540, device='cuda:0') 10000\n","```\n","\n","```\n","RESULTS OF STANDARD CNN\n","Train accuracy of the model: 99.162 %\n","tensor(59498, device='cuda:0') 60000\n","Test accuracy of the model: 92.401 %\n","tensor(9241, device='cuda:0') 10000\n","```\n","\n"]},{"cell_type":"markdown","metadata":{"id":"di4TD6kURjqA"},"source":["### Importing required libraries and setting things up"]},{"cell_type":"code","metadata":{"id":"21j-2kB6JyD8"},"source":["import torch.nn as nn\n","import torch.nn.functional as F\n","import torch\n","\n","!pip install torchviz"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ndlXr-e4Rpjh"},"source":["### Rewriting Conv2d to implement the scale invariant convolutions\n"]},{"cell_type":"markdown","metadata":{"id":"Kw64jcc9SnNg"},"source":["#### Loading the base class"]},{"cell_type":"code","metadata":{"id":"ORbM5lGSOQOd"},"source":["from torch.nn.modules.utils import _single, _pair, _triple\n","from torch.nn.modules.conv import *\n","\n","def _reverse_repeat_tuple(t, n):\n","    r\"\"\"Reverse the order of `t` and repeat each element for `n` times.\n","    This can be used to translate padding arg used by Conv and Pooling modules\n","    to the ones used by `F.pad`.\n","    \"\"\"\n","    return tuple(x for x in reversed(t) for _ in range(n))\n","\n","class _ConvNd(Module):\n","\n","    __constants__ = ['stride', 'padding', 'dilation', 'groups',\n","                     'padding_mode', 'output_padding', 'in_channels',\n","                     'out_channels', 'kernel_size']\n","    __annotations__ = {'bias': Optional[torch.Tensor]}\n","\n","    def __init__(self, in_channels, out_channels, kernel_size, stride,\n","                 padding, dilation, transposed, output_padding,\n","                 groups, bias, padding_mode):\n","        super(_ConvNd, self).__init__()\n","        if in_channels % groups != 0:\n","            raise ValueError('in_channels must be divisible by groups')\n","        if out_channels % groups != 0:\n","            raise ValueError('out_channels must be divisible by groups')\n","        valid_padding_modes = {'zeros', 'reflect', 'replicate', 'circular'}\n","        if padding_mode not in valid_padding_modes:\n","            raise ValueError(\"padding_mode must be one of {}, but got padding_mode='{}'\".format(\n","                valid_padding_modes, padding_mode))\n","        self.in_channels = in_channels\n","        self.out_channels = out_channels\n","        self.kernel_size = kernel_size\n","        self.stride = stride\n","        self.padding = padding\n","        self.dilation = dilation\n","        self.transposed = transposed\n","        self.output_padding = output_padding\n","        self.groups = groups\n","        self.padding_mode = padding_mode\n","        # `_reversed_padding_repeated_twice` is the padding to be passed to\n","        # `F.pad` if needed (e.g., for non-zero padding types that are\n","        # implemented as two ops: padding + conv). `F.pad` accepts paddings in\n","        # reverse order than the dimension.\n","        self._reversed_padding_repeated_twice = _reverse_repeat_tuple(self.padding, 2)\n","        if transposed:\n","            self.weight = Parameter(torch.Tensor(\n","                in_channels, out_channels // groups, *kernel_size))\n","        else:\n","            self.weight = Parameter(torch.Tensor(\n","                out_channels, in_channels // groups, *kernel_size))\n","        if bias:\n","            self.bias = Parameter(torch.Tensor(out_channels))\n","        else:\n","            self.register_parameter('bias', None)\n","        self.reset_parameters()\n","\n","    def reset_parameters(self):\n","        init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n","        if self.bias is not None:\n","            fan_in, _ = init._calculate_fan_in_and_fan_out(self.weight)\n","            bound = 1 / math.sqrt(fan_in)\n","            init.uniform_(self.bias, -bound, bound)\n","\n","    def extra_repr(self):\n","        s = ('{in_channels}, {out_channels}, kernel_size={kernel_size}'\n","             ', stride={stride}')\n","        if self.padding != (0,) * len(self.padding):\n","            s += ', padding={padding}'\n","        if self.dilation != (1,) * len(self.dilation):\n","            s += ', dilation={dilation}'\n","        if self.output_padding != (0,) * len(self.output_padding):\n","            s += ', output_padding={output_padding}'\n","        if self.groups != 1:\n","            s += ', groups={groups}'\n","        if self.bias is None:\n","            s += ', bias=False'\n","        if self.padding_mode != 'zeros':\n","            s += ', padding_mode={padding_mode}'\n","        return s.format(**self.__dict__)\n","\n","    def __setstate__(self, state):\n","        super(_ConvNd, self).__setstate__(state)\n","        if not hasattr(self, 'padding_mode'):\n","            self.padding_mode = 'zeros'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"z0WYIq5PSpYY"},"source":["#### Writing out new convolutional filter. Same number of parameters but convolutions at multiple scales."]},{"cell_type":"code","metadata":{"id":"wNcy4bSNRm0Q"},"source":["#\n","class Conv2dMultiScale(_ConvNd):\n","    def __init__(self, in_channels, out_channels, kernel_size, stride=1,\n","                 padding=0, dilation=1, groups=1,\n","                 bias=True, padding_mode='zeros'):\n","        kernel_size = _pair(kernel_size)\n","        stride = _pair(stride)\n","        padding = _pair(padding)\n","        dilation = _pair(dilation)\n","        super(Conv2dMultiScale, self).__init__(\n","            in_channels, out_channels, kernel_size, stride, padding, dilation,\n","            False, _pair(0), groups, bias, padding_mode)\n","        self.scale = nn.UpsamplingBilinear2d(size=(5,5))\n","\n","    def _conv_forward(self, input, weight):\n","\n","        # Typically this is the only thing that done\n","        out1 = F.conv2d(input, weight, self.bias, self.stride,\n","                        self.padding, self.dilation, self.groups)\n","        \n","        # Upscaling the weights\n","        weight = self.scale(weight)\n","\n","        # Adjusting padding to keep same output side\n","        padding = tuple(x+1 for x in self.padding)\n","\n","        # Running convolution on the bigger scale\n","        out2 = F.conv2d(input, weight, self.bias, self.stride,\n","                        padding, self.dilation, self.groups)\n","        \n","        # Returning the result\n","        return out1 + out2\n","\n","    def forward(self, input):\n","        return self._conv_forward(input, self.weight)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UnpYqRAnM8Aq"},"source":["# Understanding sizes of CNNs, ensuring it works\n","kernelSize = 3\n","m = Conv2dMultiScale(5, 3, kernelSize, stride=1, padding=(kernelSize-1) // 2)\n","input = torch.randn(20, 5, 28, 28)\n","output = m(input)\n","output.shape"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vNFpk9HcUCkf"},"source":["### Testing it on MNIST"]},{"cell_type":"code","metadata":{"id":"vU2W11EgUGCA"},"source":["import torchvision.datasets as dsets\n","import torchvision.transforms as transforms\n","\n","num_classes = 10 # number of output classes discrete range [0,9]\n","num_epochs = 2 # number of times which the entire dataset is passed throughout the model\n","batch_size = 64  # the size of input data took for one iteration\n","lr = 1e-3 # size of step\n","\n","train_transform = transforms.Compose([\n","    transforms.ToTensor()\n","])\n","\n","test_transform = transforms.Compose([\n","    transforms.RandomAffine(degrees=0, scale=(1,1.5)),\n","    transforms.ToTensor(),\n","])\n","\n","train_data = dsets.MNIST(root = './data', train = True,\n","                        transform = train_transform, download = True)\n","\n","test_data = dsets.MNIST(root = './data', train = False,\n","                       transform = test_transform)\n","\n","train_gen = torch.utils.data.DataLoader(dataset = train_data,\n","                                             batch_size = batch_size,\n","                                             shuffle = True)\n","\n","test_gen = torch.utils.data.DataLoader(dataset = test_data,\n","                                      batch_size = batch_size, \n","                                      shuffle = False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"25WykbLSM74b"},"source":["class Net(nn.Module):\n","    def __init__(self, multiScale=True):\n","        super(Net, self).__init__()\n","        self.multiScale = multiScale\n","        if(multiScale):\n","          self.conv1 = Conv2dMultiScale(1, 32, 3, 1)\n","          self.conv2 = Conv2dMultiScale(32, 64, 3, 1)\n","        else:\n","          self.conv1 = nn.Conv2d(1, 32, 3, 1)\n","          self.conv2 = nn.Conv2d(32, 64, 3, 1)\n","        self.fc = nn.Linear(9216, 10)\n","\n","    def forward(self, x):\n","        x = self.conv1(x)\n","        x = F.relu(x)\n","        x = self.conv2(x)\n","        x = F.relu(x)\n","        x = F.max_pool2d(x, 2)\n","        x = torch.flatten(x, 1)\n","        x = self.fc(x)\n","        output = F.log_softmax(x, dim=1)\n","        return output\n","\n","\n","net = Net(multiScale=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"au_aM3ouNp8Q"},"source":["if torch.cuda.is_available():\n","  net.cuda()\n","loss_function = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam( net.parameters(), lr=lr)\n","\n","for epoch in range(num_epochs):\n","  for i ,(images,labels) in enumerate(train_gen):\n","    if torch.cuda.is_available():\n","      images = images.cuda()\n","      labels = labels.cuda()\n","    \n","    optimizer.zero_grad()\n","    outputs = net(images)\n","    loss = loss_function(outputs, labels)\n","    loss.backward()\n","    optimizer.step()\n","    \n","    if (i+1) % 100 == 0:\n","      print('Epoch [%d/%d], Step [%d/%d], Loss: %.4f'\n","                 %(epoch+1, num_epochs, i+1, len(train_data)//batch_size, loss.item()))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rM64k5QvTsPQ"},"source":["if(net.multiScale):\n","  print('RESULTS OF MULTISCALE CNN')\n","else:\n","  print('RESULTS OF STANDARD CNN')\n","\n","correct = 0\n","total = 0\n","\n","for images,labels in train_gen:\n","  if torch.cuda.is_available():\n","    images = images.cuda()\n","    labels = labels.cuda()\n","  \n","  output = net(images)\n","  _, predicted = torch.max(output,1)\n","  correct += (predicted == labels).sum()\n","  total += labels.size(0)\n","train_acc = (100*correct.cpu().numpy())/(total+1)\n","print('Train accuracy of the model: %.3f %%' %(train_acc))\n","print(correct, total)\n","\n","correct = 0\n","total = 0\n","for images,labels in test_gen:\n","  if torch.cuda.is_available():\n","    images = images.cuda()\n","    labels = labels.cuda()\n","  \n","  output = net(images)\n","  _, predicted = torch.max(output,1)\n","  correct += (predicted == labels).sum()\n","  total += labels.size(0)\n","test_acc = (100*correct.cpu().numpy())/(total+1)\n","print('Test accuracy of the model: %.3f %%' %(test_acc))\n","print(correct, total)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lp690s02VZM9"},"source":[""],"execution_count":null,"outputs":[]}]}