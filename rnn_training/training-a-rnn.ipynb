{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"training-a-rnn.ipynb","provenance":[],"authorship_tag":"ABX9TyNZMIp6fUIHgCUUXc04Tr3Z"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"hsx6L1dFvY3B","colab_type":"text"},"source":["## Training RNN\n","\n","What makes this complicated is that\n","- use pytorch's default initialization (any other standard techniques won't work)\n","- use Adam (SGD is very shitty, doesn't even converge often)\n","- use tanh if using vanilla RNN, relu has issues.\n","- use bigger capacity network (increased number of hidden cells from 2 to 5; even though 2 is enough to learn the task)."]},{"cell_type":"code","metadata":{"id":"W9KPNsAbjhnF","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1595163376984,"user_tz":240,"elapsed":3334,"user":{"displayName":"Surya Penmetsa","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjWxpTxtf0SQnw5KARxI5OMEi_oD8jevaSVw1gbkg=s64","userId":"00942366501856843624"}},"outputId":"e9b5b4f5-33b5-4e8d-b1f9-0309a09069b3"},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import matplotlib.pyplot as plt\n","import numpy as np\n","from time import time\n","torch.manual_seed(1)"],"execution_count":2,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<torch._C.Generator at 0x7fde1b59adb0>"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"code","metadata":{"id":"wnLrPgxcjnTd","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1595163376986,"user_tz":240,"elapsed":3316,"user":{"displayName":"Surya Penmetsa","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjWxpTxtf0SQnw5KARxI5OMEi_oD8jevaSVw1gbkg=s64","userId":"00942366501856843624"}}},"source":["class RNNNet2(nn.Module):\n","  def __init__(self, in_size=2, hidden_size=5, out_size=2, style='lstm'):\n","    \"\"\"\n","    in_size -> state space goes in here, so size of the state features\n","    hidden_size -> hidden vector size in the LSTM\n","    out_size -> number of actions\n","    \"\"\"\n","    super(RNNNet2, self).__init__()\n","    self.hidden_size = hidden_size\n","    self.style = style\n","    if(style=='lstm'):\n","      self.rnn = nn.LSTM(input_size=in_size, hidden_size=hidden_size, num_layers=1, batch_first=True)\n","    elif(style=='rnn'):\n","      self.rnn = nn.RNN(input_size=in_size, hidden_size=hidden_size, num_layers=1, batch_first=True, nonlinearity='tanh')\n","    self.fc_head = nn.Linear(hidden_size, out_size)\n","    # for p in list(self.parameters()): # initialize all parameters to 0\n","    #   p.data.fill_(np.random.rand())\n","   \n","    self.hidden_state = torch.zeros(1, 1, hidden_size) # layer x bs x hidden_dims\n","    \n","  def reset_hidden_states(self):\n","    return torch.zeros(1, 1, self.hidden_size) # layer x bs x hidden_dims\n","  \n","  def forward(self, x): \n","    \"\"\"\n","    x -> timesteps x num_features\n","    \"\"\"\n","    if(self.style=='lstm'):\n","      hidden_state =  (torch.zeros(1, 1, self.hidden_size),torch.zeros(1, 1, self.hidden_size))\n","    else:\n","      hidden_state =  torch.zeros(1, 1, self.hidden_size)\n","    x = x.view(1, -1, x.size(-1)) # reshape to include batch\n","    # phi = phi.view(1, phi.size(0), phi.size(1)) # seq_len, batch, feature\n","    y, _ = self.rnn(x, hidden_state)  # y-> seq_len, batch, input_size      \n","    y = y.squeeze(0)\n","    q = self.fc_head(y)\n","    return q"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"TI5wyjJGjotW","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":354},"executionInfo":{"status":"ok","timestamp":1595163655429,"user_tz":240,"elapsed":2298,"user":{"displayName":"Surya Penmetsa","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjWxpTxtf0SQnw5KARxI5OMEi_oD8jevaSVw1gbkg=s64","userId":"00942366501856843624"}},"outputId":"15048aa2-7804-430c-ce63-7c0931fbd32b"},"source":["# datapoint 1\n","# x_1 = [1, 0]; y_1 = [1, 1]\n","# x_2 = [1, 1]; y_2 = [1, -1]\n","\n","# datapoint 2\n","# x_1 = [0, 1]; y_1 = [1, 1]\n","# x_2 = [1, 1]; y_2 = [-1, 1]\n","\n","x_data = torch.Tensor([[[1,0],[1,1]],\n","     [[0,1],[1,1]]]) # i x \n","t_data = torch.Tensor([[[1,1],[1,-1]],\n","     [[1,1],[-1,1]]])\n","rnn = RNNNet2(style='lstm')\n","optimizer = torch.optim.AdamW(rnn.parameters(), lr=0.01, weight_decay=1e-4)\n","\n","loss_vec = []\n","start = time()\n","for i in range(1000):\n","  # Choose a point randomly\n","  choice = np.random.randint(0,2)\n","  x = x_data[choice,:,:].unsqueeze(0)\n","  t = t_data[choice,:,:].unsqueeze(0)\n","\n","  out = rnn(x_data[choice,:,:])\n","  target = t_data[choice,:,:]\n","\n","  # use out-target to update paramters\n","  loss = (out-target).pow(2).mean()\n","  optimizer.zero_grad()\n","  loss.backward()\n","  optimizer.step()\n","  loss_vec.append(loss.data.numpy())\n","\n","print('time taken (seconds): ', time()-start)\n","plt.plot(loss_vec)\n","print(rnn(x_data[0,:,:]))\n","print(rnn(x_data[1,:,:]))"],"execution_count":11,"outputs":[{"output_type":"stream","text":["time taken (seconds):  1.3561792373657227\n","tensor([[ 1.0000,  1.0000],\n","        [ 1.0000, -1.0000]], grad_fn=<AddmmBackward>)\n","tensor([[ 1.0000,  1.0000],\n","        [-1.0000,  1.0000]], grad_fn=<AddmmBackward>)\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"code","metadata":{"id":"H2GxJZtPjqI3","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1590778763011,"user_tz":240,"elapsed":313,"user":{"displayName":"Surya Penmetsa","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjWxpTxtf0SQnw5KARxI5OMEi_oD8jevaSVw1gbkg=s64","userId":"00942366501856843624"}},"outputId":"0c0336e4-fa91-4160-b6a2-776b70488031"},"source":["time()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1590778762.1536832"]},"metadata":{"tags":[]},"execution_count":33}]},{"cell_type":"code","metadata":{"id":"WUFO6vICj5YQ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1590778274289,"user_tz":240,"elapsed":708,"user":{"displayName":"Surya Penmetsa","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjWxpTxtf0SQnw5KARxI5OMEi_oD8jevaSVw1gbkg=s64","userId":"00942366501856843624"}},"outputId":"c6aa5251-fa63-4f64-d994-1b3791e92e1c"},"source":["# Lab 12 RNN\n","import sys\n","import torch\n","import torch.nn as nn\n","from torch.autograd import Variable\n","\n","torch.manual_seed(777)  # reproducibility\n","#            0    1    2    3    4\n","idx2char = ['h', 'i', 'e', 'l', 'o']\n","\n","# Teach hihell -> ihello\n","x_data = [0, 1, 0, 2, 3, 3]   # hihell\n","one_hot_lookup = [[1, 0, 0, 0, 0],  # 0\n","                  [0, 1, 0, 0, 0],  # 1\n","                  [0, 0, 1, 0, 0],  # 2\n","                  [0, 0, 0, 1, 0],  # 3\n","                  [0, 0, 0, 0, 1]]  # 4\n","\n","y_data = [1, 0, 2, 3, 3, 4]    # ihello\n","x_one_hot = [one_hot_lookup[x] for x in x_data]\n","\n","# As we have one batch of samples, we will change them to variables only once\n","inputs = Variable(torch.Tensor(x_one_hot))\n","labels = Variable(torch.LongTensor(y_data))\n","\n","num_classes = 5\n","input_size = 5  # one-hot size\n","hidden_size = 5  # output from the RNN. 5 to directly predict one-hot\n","batch_size = 1   # one sentence\n","sequence_length = 1  # One by one\n","num_layers = 1  # one-layer rnn\n","\n","\n","class Model(nn.Module):\n","\n","    def __init__(self):\n","        super(Model, self).__init__()\n","        self.rnn = nn.RNN(input_size=input_size,\n","                          hidden_size=hidden_size, batch_first=True)\n","\n","    def forward(self, hidden, x):\n","        # Reshape input (batch first)\n","        x = x.view(batch_size, sequence_length, input_size)\n","\n","        # Propagate input through RNN\n","        # Input: (batch, seq_len, input_size)\n","        # hidden: (num_layers * num_directions, batch, hidden_size)\n","        out, hidden = self.rnn(x, hidden)\n","        return hidden, out.view(-1, num_classes)\n","\n","    def init_hidden(self):\n","        # Initialize hidden and cell states\n","        # (num_layers * num_directions, batch, hidden_size)\n","        return Variable(torch.zeros(num_layers, batch_size, hidden_size))\n","\n","\n","# Instantiate RNN model\n","model = Model()\n","print(model)\n","\n","# Set loss and optimizer function\n","# CrossEntropyLoss = LogSoftmax + NLLLoss\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n","\n","# Train the model\n","for epoch in range(100):\n","    optimizer.zero_grad()\n","    loss = 0\n","    hidden = model.init_hidden()\n","\n","    sys.stdout.write(\"predicted string: \")\n","    for input, label in zip(inputs, labels):\n","        # print(input.size(), label.size())\n","        hidden, output = model(hidden, input)\n","        if(epoch%10==0):\n","          print(output)\n","        val, idx = output.max(1)\n","        sys.stdout.write(idx2char[idx.data[0]])\n","        loss += criterion(output, label.unsqueeze(0))\n","\n","    print(\", epoch: %d, loss: %1.3f\" % (epoch + 1, loss.data.numpy()))\n","\n","    loss.backward()\n","    optimizer.step()\n","\n","print(\"Learning finished!\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model(\n","  (rnn): RNN(5, 5, batch_first=True)\n",")\n","predicted string: tensor([[ 0.3700, -0.3483, -0.1920,  0.8472,  0.5825]], grad_fn=<ViewBackward>)\n","ltensor([[ 0.3980, -0.2673,  0.0557,  0.7950, -0.1108]], grad_fn=<ViewBackward>)\n","ltensor([[ 0.2075, -0.0678, -0.0524,  0.8995,  0.3951]], grad_fn=<ViewBackward>)\n","ltensor([[ 0.3693, -0.3595, -0.3093,  0.6169,  0.0625]], grad_fn=<ViewBackward>)\n","ltensor([[ 0.4521, -0.2301,  0.1093,  0.8236, -0.0534]], grad_fn=<ViewBackward>)\n","ltensor([[ 0.4576, -0.1116,  0.2538,  0.8705, -0.2188]], grad_fn=<ViewBackward>)\n","l, epoch: 1, loss: 10.155\n","predicted string: llllll, epoch: 2, loss: 9.137\n","predicted string: llllll, epoch: 3, loss: 8.355\n","predicted string: llllll, epoch: 4, loss: 7.577\n","predicted string: llllll, epoch: 5, loss: 6.876\n","predicted string: lhelll, epoch: 6, loss: 6.327\n","predicted string: ihelll, epoch: 7, loss: 6.014\n","predicted string: ihelll, epoch: 8, loss: 5.787\n","predicted string: ihelll, epoch: 9, loss: 5.477\n","predicted string: ihelll, epoch: 10, loss: 5.274\n","predicted string: tensor([[-0.2681,  0.5584, -0.0716,  0.1155, -0.1028]], grad_fn=<ViewBackward>)\n","itensor([[ 0.9256, -0.4867, -0.7079,  0.3728, -0.7907]], grad_fn=<ViewBackward>)\n","htensor([[-0.7665, -0.0220,  0.9212, -0.0854, -0.3910]], grad_fn=<ViewBackward>)\n","etensor([[-0.8005, -0.6572, -0.9534,  0.9474, -0.6056]], grad_fn=<ViewBackward>)\n","ltensor([[-0.9814, -0.7904, -0.9764,  0.9927,  0.9628]], grad_fn=<ViewBackward>)\n","ltensor([[-0.9061, -0.9554, -0.9994,  0.9931,  0.9925]], grad_fn=<ViewBackward>)\n","l, epoch: 11, loss: 5.041\n","predicted string: ihello, epoch: 12, loss: 4.827\n","predicted string: ihello, epoch: 13, loss: 4.676\n","predicted string: ihello, epoch: 14, loss: 4.550\n","predicted string: ihello, epoch: 15, loss: 4.430\n","predicted string: ihello, epoch: 16, loss: 4.305\n","predicted string: ihello, epoch: 17, loss: 4.164\n","predicted string: ihelll, epoch: 18, loss: 4.003\n","predicted string: ihelll, epoch: 19, loss: 3.860\n","predicted string: ihelll, epoch: 20, loss: 3.879\n","predicted string: tensor([[-0.4906,  0.5845, -0.7044, -0.7153, -0.8743]], grad_fn=<ViewBackward>)\n","itensor([[ 0.9825, -0.7220, -0.9513, -0.7853, -0.9963]], grad_fn=<ViewBackward>)\n","htensor([[-0.8993, -0.7701,  0.9772, -0.9952, -0.9947]], grad_fn=<ViewBackward>)\n","etensor([[-0.9884, -0.8392, -0.9939,  0.9939, -0.9873]], grad_fn=<ViewBackward>)\n","ltensor([[-0.9996, -0.9947, -0.9963,  0.9927, -0.1752]], grad_fn=<ViewBackward>)\n","ltensor([[-0.9984, -0.9937, -0.9997,  0.9855,  0.7051]], grad_fn=<ViewBackward>)\n","l, epoch: 21, loss: 3.768\n","predicted string: ihelll, epoch: 22, loss: 3.642\n","predicted string: ihelll, epoch: 23, loss: 3.599\n","predicted string: ihello, epoch: 24, loss: 3.577\n","predicted string: ihello, epoch: 25, loss: 3.544\n","predicted string: ihello, epoch: 26, loss: 3.498\n","predicted string: ihello, epoch: 27, loss: 3.439\n","predicted string: ihello, epoch: 28, loss: 3.371\n","predicted string: ihello, epoch: 29, loss: 3.303\n","predicted string: ihello, epoch: 30, loss: 3.240\n","predicted string: tensor([[-0.8393,  0.8792, -0.9500, -0.9287, -0.9355]], grad_fn=<ViewBackward>)\n","itensor([[ 0.9962, -0.8732, -0.9993, -0.9603, -0.9990]], grad_fn=<ViewBackward>)\n","htensor([[-0.9909, -0.9561,  0.9774, -0.9996, -0.9989]], grad_fn=<ViewBackward>)\n","etensor([[-0.9984, -0.9180, -0.9993,  0.9990, -0.9973]], grad_fn=<ViewBackward>)\n","ltensor([[-0.9999, -0.9995, -0.9991,  0.7679, -0.1208]], grad_fn=<ViewBackward>)\n","ltensor([[-0.9993, -0.9965, -0.9999,  0.1500,  0.8846]], grad_fn=<ViewBackward>)\n","o, epoch: 31, loss: 3.162\n","predicted string: ihello, epoch: 32, loss: 3.147\n","predicted string: ihello, epoch: 33, loss: 3.178\n","predicted string: ihello, epoch: 34, loss: 3.116\n","predicted string: ihello, epoch: 35, loss: 3.042\n","predicted string: ihello, epoch: 36, loss: 3.020\n","predicted string: ihello, epoch: 37, loss: 3.015\n","predicted string: ihello, epoch: 38, loss: 2.998\n","predicted string: ihello, epoch: 39, loss: 2.977\n","predicted string: ihello, epoch: 40, loss: 2.966\n","predicted string: tensor([[-0.9294,  0.9584, -0.9764, -0.9642, -0.9501]], grad_fn=<ViewBackward>)\n","itensor([[ 0.9978, -0.9699, -0.9998, -0.9408, -0.9994]], grad_fn=<ViewBackward>)\n","htensor([[-0.9984, -0.9807,  0.9921, -0.9993, -0.9993]], grad_fn=<ViewBackward>)\n","etensor([[-0.9993, -0.9497, -0.9996,  0.9998, -0.9986]], grad_fn=<ViewBackward>)\n","ltensor([[-1.0000, -0.9999, -0.9993,  0.9267,  0.4110]], grad_fn=<ViewBackward>)\n","ltensor([[-0.9990, -0.9929, -1.0000, -0.6521,  0.9966]], grad_fn=<ViewBackward>)\n","o, epoch: 41, loss: 2.961\n","predicted string: ihello, epoch: 42, loss: 2.950\n","predicted string: ihello, epoch: 43, loss: 2.930\n","predicted string: ihello, epoch: 44, loss: 2.904\n","predicted string: ihello, epoch: 45, loss: 2.888\n","predicted string: ihello, epoch: 46, loss: 2.888\n","predicted string: ihello, epoch: 47, loss: 2.879\n","predicted string: ihello, epoch: 48, loss: 2.860\n","predicted string: ihello, epoch: 49, loss: 2.857\n","predicted string: ihello, epoch: 50, loss: 2.859\n","predicted string: tensor([[-0.9598,  0.9779, -0.9825, -0.9829, -0.9701]], grad_fn=<ViewBackward>)\n","itensor([[ 0.9981, -0.9876, -0.9999, -0.9461, -0.9997]], grad_fn=<ViewBackward>)\n","htensor([[-0.9993, -0.9846,  0.9965, -0.9991, -0.9997]], grad_fn=<ViewBackward>)\n","etensor([[-0.9996, -0.9669, -0.9997,  0.9999, -0.9994]], grad_fn=<ViewBackward>)\n","ltensor([[-1.0000, -0.9999, -0.9992,  0.8710,  0.0192]], grad_fn=<ViewBackward>)\n","ltensor([[-0.9997, -0.9979, -1.0000, -0.8423,  0.9777]], grad_fn=<ViewBackward>)\n","o, epoch: 51, loss: 2.852\n","predicted string: ihello, epoch: 52, loss: 2.840\n","predicted string: ihello, epoch: 53, loss: 2.834\n","predicted string: ihello, epoch: 54, loss: 2.834\n","predicted string: ihello, epoch: 55, loss: 2.824\n","predicted string: ihello, epoch: 56, loss: 2.817\n","predicted string: ihello, epoch: 57, loss: 2.817\n","predicted string: ihello, epoch: 58, loss: 2.814\n","predicted string: ihello, epoch: 59, loss: 2.808\n","predicted string: ihello, epoch: 60, loss: 2.805\n","predicted string: tensor([[-0.9723,  0.9845, -0.9850, -0.9876, -0.9763]], grad_fn=<ViewBackward>)\n","itensor([[ 0.9983, -0.9921, -0.9999, -0.9502, -0.9998]], grad_fn=<ViewBackward>)\n","htensor([[-0.9995, -0.9862,  0.9978, -0.9983, -0.9998]], grad_fn=<ViewBackward>)\n","etensor([[-0.9997, -0.9775, -0.9997,  1.0000, -0.9996]], grad_fn=<ViewBackward>)\n","ltensor([[-1.0000, -1.0000, -0.9992,  0.9621, -0.0986]], grad_fn=<ViewBackward>)\n","ltensor([[-0.9999, -0.9989, -1.0000, -0.6516,  0.9654]], grad_fn=<ViewBackward>)\n","o, epoch: 61, loss: 2.805\n","predicted string: ihello, epoch: 62, loss: 2.801\n","predicted string: ihello, epoch: 63, loss: 2.796\n","predicted string: ihello, epoch: 64, loss: 2.795\n","predicted string: ihello, epoch: 65, loss: 2.793\n","predicted string: ihello, epoch: 66, loss: 2.789\n","predicted string: ihello, epoch: 67, loss: 2.786\n","predicted string: ihello, epoch: 68, loss: 2.786\n","predicted string: ihello, epoch: 69, loss: 2.783\n","predicted string: ihello, epoch: 70, loss: 2.780\n","predicted string: tensor([[-0.9786,  0.9874, -0.9865, -0.9917, -0.9787]], grad_fn=<ViewBackward>)\n","itensor([[ 0.9984, -0.9937, -0.9999, -0.9729, -0.9999]], grad_fn=<ViewBackward>)\n","htensor([[-0.9996, -0.9872,  0.9983, -0.9980, -0.9999]], grad_fn=<ViewBackward>)\n","etensor([[-0.9997, -0.9839, -0.9997,  1.0000, -0.9997]], grad_fn=<ViewBackward>)\n","ltensor([[-1.0000, -1.0000, -0.9991,  0.9505, -0.0904]], grad_fn=<ViewBackward>)\n","ltensor([[-0.9999, -0.9989, -1.0000, -0.8606,  0.9735]], grad_fn=<ViewBackward>)\n","o, epoch: 71, loss: 2.780\n","predicted string: ihello, epoch: 72, loss: 2.778\n","predicted string: ihello, epoch: 73, loss: 2.776\n","predicted string: ihello, epoch: 74, loss: 2.775\n","predicted string: ihello, epoch: 75, loss: 2.774\n","predicted string: ihello, epoch: 76, loss: 2.772\n","predicted string: ihello, epoch: 77, loss: 2.770\n","predicted string: ihello, epoch: 78, loss: 2.769\n","predicted string: ihello, epoch: 79, loss: 2.768\n","predicted string: ihello, epoch: 80, loss: 2.766\n","predicted string: tensor([[-0.9823,  0.9890, -0.9877, -0.9934, -0.9811]], grad_fn=<ViewBackward>)\n","itensor([[ 0.9986, -0.9946, -0.9999, -0.9806, -0.9999]], grad_fn=<ViewBackward>)\n","htensor([[-0.9997, -0.9884,  0.9986, -0.9973, -0.9999]], grad_fn=<ViewBackward>)\n","etensor([[-0.9997, -0.9875, -0.9997,  1.0000, -0.9998]], grad_fn=<ViewBackward>)\n","ltensor([[-1.0000, -1.0000, -0.9991,  0.9641, -0.1620]], grad_fn=<ViewBackward>)\n","ltensor([[-0.9999, -0.9993, -1.0000, -0.8145,  0.9656]], grad_fn=<ViewBackward>)\n","o, epoch: 81, loss: 2.765\n","predicted string: ihello, epoch: 82, loss: 2.764\n","predicted string: ihello, epoch: 83, loss: 2.763\n","predicted string: ihello, epoch: 84, loss: 2.762\n","predicted string: ihello, epoch: 85, loss: 2.761\n","predicted string: ihello, epoch: 86, loss: 2.759\n","predicted string: ihello, epoch: 87, loss: 2.759\n","predicted string: ihello, epoch: 88, loss: 2.758\n","predicted string: ihello, epoch: 89, loss: 2.757\n","predicted string: ihello, epoch: 90, loss: 2.756\n","predicted string: tensor([[-0.9849,  0.9902, -0.9886, -0.9946, -0.9823]], grad_fn=<ViewBackward>)\n","itensor([[ 0.9987, -0.9952, -0.9999, -0.9848, -0.9999]], grad_fn=<ViewBackward>)\n","htensor([[-0.9997, -0.9896,  0.9987, -0.9966, -0.9999]], grad_fn=<ViewBackward>)\n","etensor([[-0.9998, -0.9899, -0.9997,  1.0000, -0.9998]], grad_fn=<ViewBackward>)\n","ltensor([[-1.0000, -1.0000, -0.9992,  0.9683, -0.1509]], grad_fn=<ViewBackward>)\n","ltensor([[-0.9999, -0.9993, -1.0000, -0.8694,  0.9742]], grad_fn=<ViewBackward>)\n","o, epoch: 91, loss: 2.755\n","predicted string: ihello, epoch: 92, loss: 2.754\n","predicted string: ihello, epoch: 93, loss: 2.753\n","predicted string: ihello, epoch: 94, loss: 2.752\n","predicted string: ihello, epoch: 95, loss: 2.751\n","predicted string: ihello, epoch: 96, loss: 2.750\n","predicted string: ihello, epoch: 97, loss: 2.750\n","predicted string: ihello, epoch: 98, loss: 2.749\n","predicted string: ihello, epoch: 99, loss: 2.748\n","predicted string: ihello, epoch: 100, loss: 2.747\n","Learning finished!\n"],"name":"stdout"}]}]}